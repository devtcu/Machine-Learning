{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2be91b",
   "metadata": {},
   "source": [
    "Natural Language Processing deals witho processing human languages into computer language - like spellcheck or autocomplete. To do so, we will use something called a Recurren Neural Network (RNN) - **a deep learning model that is trained to process and convert a sequential data input into a specific sequential data output**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa646da",
   "metadata": {},
   "source": [
    "Up until this point we have been using something called **feed-forward** neural networks. This simply means that all our data is fed forwards (all at once) from left to right through the network. This was fine for the problems we considered before but won't work very well for processing text. After all, even we (humans) don't process text all at once. We read word by word from left to right and keep track of the current meaning of the sentence so we can understand the meaning of the next word. Well this is exaclty what a recurrent neural network is designed to do. When we say recurrent neural network all we really mean is a network that contains a loop. A RNN will process one word at a time while maintaining an internal memory of what it's already seen. This will allow it to treat words differently based on their order in a sentence and to slowly build an understanding of the entire input, one word at a time.\n",
    "\n",
    "This is why we are treating our text data as a sequence! So that we can pass one word at a time to the RNN.\n",
    "\n",
    "Let's have a look at what a recurrent layer might look like.\n",
    "\n",
    "![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
    "*Source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c6415",
   "metadata": {},
   "source": [
    "As an example, we will now do a sentiment analysis oof a movie review to see whether it was a positive or negative one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a0f7ef",
   "metadata": {},
   "source": [
    "Can you believe it, keras has a movie reviews database! Lets use it! It turns out all the words are already encoded with a number - yay for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40529f94",
   "metadata": {},
   "source": [
    "This dataset contains 25,000 reviews from IMDB where each one is already preprocessed and has a label as either positive or negative. Each review is encoded by integers that represents how common a word is in the entire dataset. For example, a word encoded by the integer 3 means that it is the 3rd most common word in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c860edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#loading in our data\n",
    "\n",
    "VOCAB_SIZE = 88584\n",
    "\n",
    "MAXLEN = 250 #max length of a review\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "#the data will be the review arrays encoded by integers, while the labels will be a 0 or 1, depending on whether it is negative or positive, respectively\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c009b8",
   "metadata": {},
   "source": [
    "Lets look at an example, the first review in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87bd6c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 47,\n",
       " 8,\n",
       " 30,\n",
       " 31,\n",
       " 7,\n",
       " 4,\n",
       " 249,\n",
       " 108,\n",
       " 7,\n",
       " 4,\n",
       " 5974,\n",
       " 54,\n",
       " 61,\n",
       " 369,\n",
       " 13,\n",
       " 71,\n",
       " 149,\n",
       " 14,\n",
       " 22,\n",
       " 112,\n",
       " 4,\n",
       " 2401,\n",
       " 311,\n",
       " 12,\n",
       " 16,\n",
       " 3711,\n",
       " 33,\n",
       " 75,\n",
       " 43,\n",
       " 1829,\n",
       " 296,\n",
       " 4,\n",
       " 86,\n",
       " 320,\n",
       " 35,\n",
       " 534,\n",
       " 19,\n",
       " 263,\n",
       " 4821,\n",
       " 1301,\n",
       " 4,\n",
       " 1873,\n",
       " 33,\n",
       " 89,\n",
       " 78,\n",
       " 12,\n",
       " 66,\n",
       " 16,\n",
       " 4,\n",
       " 360,\n",
       " 7,\n",
       " 4,\n",
       " 58,\n",
       " 316,\n",
       " 334,\n",
       " 11,\n",
       " 4,\n",
       " 1716,\n",
       " 43,\n",
       " 645,\n",
       " 662,\n",
       " 8,\n",
       " 257,\n",
       " 85,\n",
       " 1200,\n",
       " 42,\n",
       " 1228,\n",
       " 2578,\n",
       " 83,\n",
       " 68,\n",
       " 3912,\n",
       " 15,\n",
       " 36,\n",
       " 165,\n",
       " 1539,\n",
       " 278,\n",
       " 36,\n",
       " 69,\n",
       " 44076,\n",
       " 780,\n",
       " 8,\n",
       " 106,\n",
       " 14,\n",
       " 6905,\n",
       " 1338,\n",
       " 18,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 610,\n",
       " 40,\n",
       " 6,\n",
       " 87,\n",
       " 326,\n",
       " 23,\n",
       " 2300,\n",
       " 21,\n",
       " 23,\n",
       " 22,\n",
       " 12,\n",
       " 272,\n",
       " 40,\n",
       " 57,\n",
       " 31,\n",
       " 11,\n",
       " 4,\n",
       " 22,\n",
       " 47,\n",
       " 6,\n",
       " 2307,\n",
       " 51,\n",
       " 9,\n",
       " 170,\n",
       " 23,\n",
       " 595,\n",
       " 116,\n",
       " 595,\n",
       " 1352,\n",
       " 13,\n",
       " 191,\n",
       " 79,\n",
       " 638,\n",
       " 89,\n",
       " 51428,\n",
       " 14,\n",
       " 9,\n",
       " 8,\n",
       " 106,\n",
       " 607,\n",
       " 624,\n",
       " 35,\n",
       " 534,\n",
       " 6,\n",
       " 227,\n",
       " 7,\n",
       " 129,\n",
       " 113]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d526649",
   "metadata": {},
   "source": [
    "All the 25000 reviews are of different lengths. This is a problem since the same length data must be passed into a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea22cb5",
   "metadata": {},
   "source": [
    "- if the review is greater than 250 words then trim off the extra words\n",
    "- if the review is less than 250 words add the necessary amount of 0's to make it equal to 250.\n",
    "\n",
    "Luckily for us keras has a function that can do this for us (this is called padding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42ff2819",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
    "test_data = sequence.pad_sequences(test_data, MAXLEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbf3f5",
   "metadata": {},
   "source": [
    "Now let us create out model for the data. Our first layer will be a word embedding layer and then a LSTM layer afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36084474",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(VOCAB_SIZE, 32), #The Embedding layer converts each word index into a 32-dimensional vector, allowing the model to learn useful representations of words during training.\n",
    "    \n",
    "    tf.keras.layers.LSTM(32), #when we pass the embdedding to LSTM, we have to tell it that it has 32 dimesnsions for every single word\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e6eb41",
   "metadata": {},
   "source": [
    "We are trying the predict the sentiment of the review. If we have the sentiment between 0 and 1, and then if the review outputs a number greater than 0.5, we can classify it as a positive review. the activation function __sigmoid__ is perfect for this since it squishes all values in between 0 and 1 so that we can make an accurate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc2dfce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b09d3d3",
   "metadata": {},
   "source": [
    "Now Let us train our model! model.compile is creating the loss functions, optimizer, and metrics that we would like to track. Binary crossentropy tells us how far away we are from the correct 0 or 1 value. Could have used 'adam' for opitimizer. the 0.2 in validation_split means that 20% of the training data is set aside for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c31c73c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 35ms/step - acc: 0.6679 - loss: 0.5753 - val_acc: 0.8662 - val_loss: 0.3263\n",
      "Epoch 2/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - acc: 0.8849 - loss: 0.2869 - val_acc: 0.8830 - val_loss: 0.3160\n",
      "Epoch 3/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - acc: 0.9113 - loss: 0.2362 - val_acc: 0.8870 - val_loss: 0.2948\n",
      "Epoch 4/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 35ms/step - acc: 0.9326 - loss: 0.1869 - val_acc: 0.8702 - val_loss: 0.3125\n",
      "Epoch 5/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - acc: 0.9459 - loss: 0.1565 - val_acc: 0.8812 - val_loss: 0.3015\n",
      "Epoch 6/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 35ms/step - acc: 0.9551 - loss: 0.1322 - val_acc: 0.8758 - val_loss: 0.3122\n",
      "Epoch 7/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 35ms/step - acc: 0.9628 - loss: 0.1162 - val_acc: 0.8570 - val_loss: 0.4187\n",
      "Epoch 8/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 34ms/step - acc: 0.9675 - loss: 0.1031 - val_acc: 0.8786 - val_loss: 0.3823\n",
      "Epoch 9/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - acc: 0.9700 - loss: 0.0879 - val_acc: 0.8802 - val_loss: 0.3493\n",
      "Epoch 10/10\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 34ms/step - acc: 0.9764 - loss: 0.0709 - val_acc: 0.8744 - val_loss: 0.4667\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f11da4",
   "metadata": {},
   "source": [
    "Now lets use the untouched testing dataset with another 25000 reviews that our model has never seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1bcbd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - acc: 0.8516 - loss: 0.5490\n",
      "[0.5456685423851013, 0.8544800281524658]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3bc6df",
   "metadata": {},
   "source": [
    "How about we try and make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087650e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 6 3 7 2 5 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create the vectorization layer\n",
    "vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_sequence_length=MAXLEN\n",
    ")\n",
    "# Adapt the vectorizer to the IMDB word index vocabulary\n",
    "# You can adapt on your own text data or a sample list of texts\n",
    "vectorizer.adapt([\"that movie was just amazing, so amazing\"])\n",
    "\n",
    "def encode_text(text):\n",
    "    # The vectorizer will tokenize, index, and pad automatically\n",
    "    return vectorizer([text])[0].numpy()\n",
    "\n",
    "text = \"that movie was just amazing, so amazing\"\n",
    "encoded = encode_text(text)\n",
    "print(encoded) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
